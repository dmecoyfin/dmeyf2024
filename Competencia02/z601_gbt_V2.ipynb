{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1Vd-Rfyik62j"
   },
   "source": [
    "# Gradient Boosting Desicion Tree\n",
    "\n",
    "En las clases anteriores, observamos cómo las mejoras en los algoritmos y las optimizaciones pueden generar avances significativos en la ganancia. Ya hemos logrado un progreso considerable con los modelos de Random Forest. Hoy, daremos un paso aún más grande al explorar los modelos que actualmente están obteniendo los mejores resultados en este tipo de dominios.\n",
    "\n",
    "Antes que nada, carguemos el entorno de trabajo\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: pip in /usr/lib/python3/dist-packages (24.0)\n",
      "Collecting pip\n",
      "  Using cached pip-24.3.1-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: setuptools in /usr/lib/python3/dist-packages (68.1.2)\n",
      "Collecting setuptools\n",
      "  Using cached setuptools-75.6.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: wheel in /usr/lib/python3/dist-packages (0.42.0)\n",
      "Collecting wheel\n",
      "  Using cached wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Using cached pip-24.3.1-py3-none-any.whl (1.8 MB)\n",
      "Using cached setuptools-75.6.0-py3-none-any.whl (1.2 MB)\n",
      "Using cached wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "Installing collected packages: wheel, setuptools, pip\n",
      "Successfully installed pip-24.3.1 setuptools-75.6.0 wheel-0.45.1\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install --upgrade pip setuptools wheel --break-system-packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: seaborn==0.13.1 in /home/fernandomrestelli/.local/lib/python3.12/site-packages (0.13.1)\n",
      "Requirement already satisfied: numpy==1.26.4 in /usr/lib/python3/dist-packages (1.26.4)\n",
      "Requirement already satisfied: matplotlib==3.7.1 in /home/fernandomrestelli/.local/lib/python3.12/site-packages (3.7.1)\n",
      "Requirement already satisfied: pandas==2.1.4 in /home/fernandomrestelli/.local/lib/python3.12/site-packages (2.1.4)\n",
      "Requirement already satisfied: lightgbm==4.4.0 in /home/fernandomrestelli/.local/lib/python3.12/site-packages (4.4.0)\n",
      "Requirement already satisfied: optuna==3.6 in /home/fernandomrestelli/.local/lib/python3.12/site-packages (3.6.0)\n",
      "Collecting scikit-learn==1.3.1\n",
      "  Downloading scikit_learn-1.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (11 kB)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /usr/lib/python3/dist-packages (from matplotlib==3.7.1) (1.0.7)\n",
      "Requirement already satisfied: cycler>=0.10 in /usr/lib/python3/dist-packages (from matplotlib==3.7.1) (0.11.0)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /usr/lib/python3/dist-packages (from matplotlib==3.7.1) (4.46.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /home/fernandomrestelli/.local/lib/python3.12/site-packages (from matplotlib==3.7.1) (1.4.7)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/fernandomrestelli/.local/lib/python3.12/site-packages (from matplotlib==3.7.1) (24.2)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /usr/lib/python3/dist-packages (from matplotlib==3.7.1) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /usr/lib/python3/dist-packages (from matplotlib==3.7.1) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /home/fernandomrestelli/.local/lib/python3.12/site-packages (from matplotlib==3.7.1) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/lib/python3/dist-packages (from pandas==2.1.4) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/fernandomrestelli/.local/lib/python3.12/site-packages (from pandas==2.1.4) (2024.2)\n",
      "Requirement already satisfied: scipy in /usr/lib/python3/dist-packages (from lightgbm==4.4.0) (1.11.4)\n",
      "Requirement already satisfied: alembic>=1.5.0 in /home/fernandomrestelli/.local/lib/python3.12/site-packages (from optuna==3.6) (1.14.0)\n",
      "Requirement already satisfied: colorlog in /home/fernandomrestelli/.local/lib/python3.12/site-packages (from optuna==3.6) (6.9.0)\n",
      "Requirement already satisfied: sqlalchemy>=1.3.0 in /home/fernandomrestelli/.local/lib/python3.12/site-packages (from optuna==3.6) (2.0.36)\n",
      "Requirement already satisfied: tqdm in /home/fernandomrestelli/.local/lib/python3.12/site-packages (from optuna==3.6) (4.67.0)\n",
      "Requirement already satisfied: PyYAML in /usr/lib/python3/dist-packages (from optuna==3.6) (6.0.1)\n",
      "Collecting joblib>=1.1.1 (from scikit-learn==1.3.1)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting threadpoolctl>=2.0.0 (from scikit-learn==1.3.1)\n",
      "  Using cached threadpoolctl-3.5.0-py3-none-any.whl.metadata (13 kB)\n",
      "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna==3.6) (1.3.2.dev0)\n",
      "Requirement already satisfied: typing-extensions>=4 in /home/fernandomrestelli/.local/lib/python3.12/site-packages (from alembic>=1.5.0->optuna==3.6) (4.12.2)\n",
      "Requirement already satisfied: six>=1.5 in /home/fernandomrestelli/.local/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib==3.7.1) (1.16.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/fernandomrestelli/.local/lib/python3.12/site-packages (from sqlalchemy>=1.3.0->optuna==3.6) (3.1.1)\n",
      "Downloading scikit_learn-1.3.1-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached threadpoolctl-3.5.0-py3-none-any.whl (18 kB)\n",
      "Installing collected packages: threadpoolctl, joblib, scikit-learn\n",
      "Successfully installed joblib-1.4.2 scikit-learn-1.3.1 threadpoolctl-3.5.0\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install seaborn==0.13.1 numpy==1.26.4 matplotlib==3.7.1 pandas==2.1.4 lightgbm==4.4.0 optuna==3.6 scikit-learn==1.3.1 --break-system-packages\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "Cj-rL6xHlA2u"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/fernandomrestelli/.local/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import ShuffleSplit, StratifiedShuffleSplit\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.impute import SimpleImputer\n",
    "\n",
    "import lightgbm as lgb\n",
    "\n",
    "import optuna\n",
    "from optuna.visualization import plot_optimization_history, plot_param_importances, plot_slice, plot_contour\n",
    "\n",
    "from time import time\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8jGKjoN1lRho"
   },
   "outputs": [],
   "source": [
    "base_path = '/home/fernandomrestelli/dmeyf2024/Competencia02/'\n",
    "dataset_path = base_path + 'datos/'\n",
    "modelos_path = base_path + 'modelos/'\n",
    "db_path = base_path + 'db/'\n",
    "dataset_file = 'competencia_02_crudo.csv'\n",
    "\n",
    "ganancia_acierto = 273000\n",
    "costo_estimulo = 7000\n",
    "\n",
    "mes_train = 202104\n",
    "mes_test = 202106\n",
    "\n",
    "# agregue sus semillas\n",
    "semillas = [945787,945799,945809,945811,945817]\n",
    "\n",
    "data = pd.read_csv(dataset_path + dataset_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mdata\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ordenar los datos por cliente y por foto_mes para asegurarse de que el cálculo de deltas sea secuencial\n",
    "data = data.sort_values(by=['numero_de_cliente', 'foto_mes'])\n",
    "\n",
    "# Seleccionar las columnas numéricas donde tiene sentido calcular deltas\n",
    "columnas_a_calcular = data.select_dtypes(include=['float64', 'int64']).columns\n",
    "\n",
    "# Excluir las columnas 'cliente' y 'foto_mes' ya que no queremos calcular deltas para estas\n",
    "columnas_a_calcular = [col for col in columnas_a_calcular if col not in ['numero_de_cliente', 'foto_mes']]\n",
    "\n",
    "# Calcular los deltas para todas las columnas numéricas\n",
    "for columna in columnas_a_calcular:\n",
    "    data[f'delta_{columna}'] = data.groupby('numero_de_cliente')[columna].diff()\n",
    "\n",
    "# Visualizar las primeras filas del dataframe con los deltas\n",
    "print(data.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9TrH9f1L5Umd"
   },
   "source": [
    "Vamos a asignar pesos a las clases. En unos minutos explicaremos las razones detrás de esta decisión. Mientras tanto, pueden aprovechar el código para ajustar el peso de la clase **BAJA+2** según lo deseen.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AlYeDIBQP3-s"
   },
   "outputs": [],
   "source": [
    "data['clase_peso'] = 1.0\n",
    "\n",
    "data.loc[data['clase_ternaria'] == 'BAJA+2', 'clase_peso'] = 1.00002\n",
    "data.loc[data['clase_ternaria'] == 'BAJA+1', 'clase_peso'] = 1.00001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data\n",
    "\n",
    "data.drop(['cprestamos_personales', 'mprestamos_personales'], axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lzLIRVs850-I"
   },
   "source": [
    "Además, como se mencionó en la clase pasada, comenzaremos a experimentar con nuevas clases para ajustar el modelo. En particular, sumaremos la clase **BAJA+1**, que es estructuralmente muy similar a **BAJA+2**, para aumentar los casos positivos. Luego, compararemos los resultados obtenidos con los de la clase con la que hemos estado trabajando hasta ahora."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KV1meQ5cZ_Sl"
   },
   "outputs": [],
   "source": [
    "data['clase_binaria1'] = 0\n",
    "data['clase_binaria2'] = 0\n",
    "data['clase_binaria1'] = np.where(data['clase_ternaria'] == 'BAJA+2', 1, 0)\n",
    "data['clase_binaria2'] = np.where(data['clase_ternaria'] == 'CONTINUA', 0, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5AYJG0r16dW9"
   },
   "source": [
    "Y trabajaremos como es habitual en las últimas clases, con **Febrero** para entrenar y **Abril** para medir, con el fin de realizar *backtesting*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "iDyeXHAuCKuT"
   },
   "outputs": [],
   "source": [
    "train_data = data[data['foto_mes'] == mes_train]\n",
    "test_data = data[data['foto_mes'] == mes_test]\n",
    "\n",
    "X_train = train_data.drop(['clase_ternaria', 'clase_peso', 'clase_binaria1','clase_binaria2'], axis=1)\n",
    "y_train_binaria1 = train_data['clase_binaria1']\n",
    "y_train_binaria2 = train_data['clase_binaria2']\n",
    "w_train = train_data['clase_peso']\n",
    "\n",
    "X_test = test_data.drop(['clase_ternaria', 'clase_peso', 'clase_binaria1','clase_binaria2'], axis=1)\n",
    "y_test_binaria1 = test_data['clase_binaria1']\n",
    "y_test_class = test_data['clase_ternaria']\n",
    "w_test = test_data['clase_peso']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "scpnp1HJ6wfO"
   },
   "source": [
    "Y preparamos el *dataset* para poder usar el **rf** de una clase anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5cqbDiI4x2OD"
   },
   "outputs": [],
   "source": [
    "imp_mean = SimpleImputer(missing_values=np.nan, strategy='mean')\n",
    "Xif = imp_mean.fit_transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k31eSe5zlTEk"
   },
   "source": [
    "Comenzaremos explicando el funcionamiento del protagonista de esta clase: **LightGBM**. Primero, partiremos con una revisión de cómo funciona el algoritmo en el que se basa, **XGBoost**. Para una introducción completa, puedes consultar este\n",
    "\n",
    "https://xgboost.readthedocs.io/en/stable/tutorials/model.html.\n",
    "\n",
    "Aunque en la cátedra no somos grandes seguidores de Josh Starmer y su canal *StatQuest*, reconozco que sus series sobre *Gradient Boosting* y *XGBoost* son excelentes recursos. Aquí te dejamos los enlaces a esas dos series que realmente valen la pena:\n",
    "\n",
    "[Serie Gradient Boosting](https://www.youtube.com/watch?v=3CC4N4z3GJc&list=PLblh5JKOoLUJjeXUvUE0maghNuY2_5fY6)\n",
    "\n",
    "[Serie XGBoost](https://www.youtube.com/watch?v=OtD8wVaFm6E&list=PLblh5JKOoLULU0irPgs1SnKO6wqVjKUsQ)\n",
    "\n",
    "Finalmente, analizaremos las diferencias clave que ofrece **LightGBM** frente a XGBoost. Puedes explorar más sobre ello en este https://lightgbm.readthedocs.io/en/stable/Features.html.\n",
    "\n",
    "No olvides tener a mano la [documentación de LightGBM](https://lightgbm.readthedocs.io/)y la [lista completa de sus parámetros](https://lightgbm.readthedocs.io/en/latest/Parameters.html).\n",
    "\n",
    "Este es un algoritmo muy usado en el mercado, recomiendo dedicarle el tiempo necesario para aprenderlo bien."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UQ-3AgzL9ude"
   },
   "source": [
    "Vamos a utilizar el algoritmo directamente, sin pasar por *scikit-learn*. Sin embargo, si algún alumno lo prefiere, puede optar por usar el *wrapper* de sklearn para este caso.\n",
    "\n",
    "Para evaluar la calidad del modelo, crearemos nuestra propia función de evaluación que calcule la ganancia. La razón de incluir los pesos es precisamente para poder implementar esta función de evaluación de manera adecuada. Al combinar las clases *BAJA+1* y *BAJA+2* en una sola, necesitamos una forma de diferenciarlas, y es aquí donde entra en juego el *weight*. Este parámetro nos permitirá distinguir entre ambas clases al momento de evaluarlas dentro del algoritmo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_FvDUXeatl66"
   },
   "outputs": [],
   "source": [
    "def lgb_gan_eval(y_pred, data):\n",
    "    weight = data.get_weight()\n",
    "    ganancia = np.where(weight == 1.00002, ganancia_acierto, 0) - np.where(weight < 1.00002, costo_estimulo, 0)\n",
    "    ganancia = ganancia[np.argsort(y_pred)[::-1]]\n",
    "    ganancia = np.cumsum(ganancia)\n",
    "\n",
    "    return 'gan_eval', np.max(ganancia) , True\n",
    "\n",
    "# Parámetros del modelos.\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'gan_eval',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'max_bin': 31,\n",
    "    'num_leaves': 31,\n",
    "    'learning_rate': 0.01,\n",
    "    'feature_fraction': 0.3,\n",
    "    'bagging_fraction': 0.7,\n",
    "    'verbose': 0\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Qte_kOcU-7i3"
   },
   "source": [
    "LGBM necesita su propio tipo de Datasets:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5dnWsRWgPnRr"
   },
   "outputs": [],
   "source": [
    "train_data1 = lgb.Dataset(X_train, label=y_train_binaria1, weight=w_train)\n",
    "train_data2 = lgb.Dataset(X_train, label=y_train_binaria2, weight=w_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3GSEwNi3_IxN"
   },
   "source": [
    "A continuación, compararemos las dos clases. Utilizaremos para medir la calidad de las clases (y de los parámetros), la función **cv** que viene *out-of-the-box*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ibXn1tiLNT6J"
   },
   "outputs": [],
   "source": [
    "cv_results1 = lgb.cv(\n",
    "    params,\n",
    "    train_data1,\n",
    "    num_boost_round=150,\n",
    "    feval=lgb_gan_eval,\n",
    "    nfold=5,\n",
    "    seed=semillas[0]\n",
    ")\n",
    "\n",
    "cv_results2 = lgb.cv(\n",
    "    params,\n",
    "    train_data2,\n",
    "    num_boost_round=150,\n",
    "    feval=lgb_gan_eval,\n",
    "    nfold=5,\n",
    "    seed=semillas[0]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ePRPrDL8_odf"
   },
   "source": [
    "Y vizualizamos los resultados de ambas ejecuciones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 565
    },
    "id": "cOoWx9sbhx5h",
    "outputId": "68d27639-4af7-4003-d04c-a3a0eb327673"
   },
   "outputs": [],
   "source": [
    "df_ganancias = pd.DataFrame({\n",
    "    'binaria1': cv_results1['valid gan_eval-mean'],\n",
    "    'binaria2': cv_results2['valid gan_eval-mean'],\n",
    "    'Iteracion': range(1, len(cv_results1['valid gan_eval-mean']) + 1)\n",
    "})\n",
    "\n",
    "# Normalizamos la ganancias\n",
    "df_ganancias['binaria1'] = df_ganancias['binaria1']*5\n",
    "df_ganancias['binaria2'] = df_ganancias['binaria2']*5\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.lineplot(x='Iteracion', y='binaria1', data=df_ganancias, label='binaria 1')\n",
    "sns.lineplot(x='Iteracion', y='binaria2', data=df_ganancias, label='binaria 2')\n",
    "plt.title('Comparación de las Ganancias de las 2 clases binarias')\n",
    "plt.xlabel('Iteración')\n",
    "plt.ylabel('Ganancia')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "N96lUJOLDqLH"
   },
   "source": [
    "Se observa una ligera mejora al combinar las clases en modelos sencillos. Dado que cada pequeña mejora es importante, continuaremos utilizando esta estrategia.\n",
    "\n",
    "A continuación, procederemos a optimizar **LightGBM** utilizando la librería **Optuna**. Cabe destacar que las optimizaciones que realizaremos son básicas y están diseñadas para ejecutarse en pocos minutos. Será su responsabilidad ampliar tanto el rango de búsqueda como el tiempo de optimización para obtener un modelo más competitivo.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bYMEnNFbkSoQ",
    "outputId": "15d768d4-9e25-4063-984b-d79c90ea91bc"
   },
   "outputs": [],
   "source": [
    "\n",
    "def objective(trial):\n",
    "\n",
    "    num_leaves = trial.suggest_int('num_leaves', 8, 100),\n",
    "    learning_rate = trial.suggest_float('learning_rate', 0.005, 0.3), # mas bajo, más iteraciones necesita\n",
    "    min_data_in_leaf = trial.suggest_int('min_data_in_leaf', 1, 1000),\n",
    "    feature_fraction = trial.suggest_float('feature_fraction', 0.1, 1.0),\n",
    "    bagging_fraction = trial.suggest_float('bagging_fraction', 0.1, 1.0),\n",
    "\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'custom',\n",
    "        'boosting_type': 'gbdt',\n",
    "        'first_metric_only': True,\n",
    "        'boost_from_average': True,\n",
    "        'feature_pre_filter': False,\n",
    "        'max_bin': 31,\n",
    "        'num_leaves': num_leaves,\n",
    "        'learning_rate': learning_rate,\n",
    "        'min_data_in_leaf': min_data_in_leaf,\n",
    "        'feature_fraction': feature_fraction,\n",
    "        'bagging_fraction': bagging_fraction,\n",
    "        'seed': semillas[0],\n",
    "        'verbose': -1\n",
    "    }\n",
    "    train_data = lgb.Dataset(X_train,\n",
    "                              label=y_train_binaria2, # eligir la clase\n",
    "                              weight=w_train)\n",
    "    cv_results = lgb.cv(\n",
    "        params,\n",
    "        train_data,\n",
    "        num_boost_round=100, # modificar, subit y subir... y descomentar la línea inferior\n",
    "        # early_stopping_rounds= int(50 + 5 / learning_rate),\n",
    "        feval=lgb_gan_eval,\n",
    "        stratified=True,\n",
    "        nfold=5,\n",
    "        seed=semillas[0]\n",
    "    )\n",
    "    max_gan = max(cv_results['valid gan_eval-mean'])\n",
    "    best_iter = cv_results['valid gan_eval-mean'].index(max_gan) + 1\n",
    "\n",
    "    # Guardamos cual es la mejor iteración del modelo\n",
    "    trial.set_user_attr(\"best_iter\", best_iter)\n",
    "\n",
    "    return max_gan * 5\n",
    "\n",
    "\n",
    "storage_name = \"sqlite:///\" + db_path + \"optimization_lgbm1.db\"\n",
    "study_name = \"exp_301_lgbm1\"\n",
    "\n",
    "study = optuna.create_study(\n",
    "    direction=\"maximize\",\n",
    "    study_name=study_name,\n",
    "    storage=storage_name,\n",
    "    load_if_exists=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AMrT22K0u9JF",
    "outputId": "75ee1c52-fb2c-4ad4-df1e-1d0852a3867e"
   },
   "outputs": [],
   "source": [
    "study.optimize(objective, n_trials=50) # subir subir"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ia2vN07FEasX"
   },
   "source": [
    "Analizamos los resultados as usual"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pip install --upgrade nbformat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "fH4ybQgYx7Xf",
    "outputId": "e2492e33-7da9-408e-bc68-ecc14c0fdf99"
   },
   "outputs": [],
   "source": [
    "import optuna.visualization as vis\n",
    "vis.plot_optimization_history(study)\n",
    "#optuna.visualization.plot_optimization_history(study)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "vOfm5DXAx8Rj",
    "outputId": "7ea05841-331e-48cb-fcb9-c36e97ab2051"
   },
   "outputs": [],
   "source": [
    "plot_param_importances(study)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O0Z-r8QYEsNN"
   },
   "source": [
    "El **learning rate** es un parámetro que tiene que ir acompañado por más árboles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 562
    },
    "id": "0U6CfznSx-gG",
    "outputId": "f3c4a028-2d47-4e6e-ba8b-2877fe9d843a"
   },
   "outputs": [],
   "source": [
    "plot_slice(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "XRqPgCD6yB_q",
    "outputId": "0adae389-2981-43ee-c939-212232dd4f31"
   },
   "outputs": [],
   "source": [
    "plot_contour(study)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 542
    },
    "id": "wGHGdGcQ3m00",
    "outputId": "624ee6d0-a5f1-4b00-bac6-8f37de7e99a7"
   },
   "outputs": [],
   "source": [
    "plot_contour(study, params=['num_leaves','min_data_in_leaf'] )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HjgD6raVE6am"
   },
   "source": [
    "Y finalmente tomamos el mejor modelo y lo entrenamos con la totalidad de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bwyUriQksZAM",
    "outputId": "c4b54b3e-75b0-4076-8a31-7096bd94382b"
   },
   "outputs": [],
   "source": [
    "best_iter = study.best_trial.user_attrs[\"best_iter\"]\n",
    "print(f\"Mejor cantidad de árboles para el mejor model {best_iter}\")\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'first_metric_only': True,\n",
    "    'boost_from_average': True,\n",
    "    'feature_pre_filter': False,\n",
    "    'max_bin': 31,\n",
    "    'num_leaves': study.best_trial.params['num_leaves'],\n",
    "    'learning_rate': study.best_trial.params['learning_rate'],\n",
    "    'min_data_in_leaf': study.best_trial.params['min_data_in_leaf'],\n",
    "    'feature_fraction': study.best_trial.params['feature_fraction'],\n",
    "    'bagging_fraction': study.best_trial.params['bagging_fraction'],\n",
    "    'seed': semillas[0],\n",
    "    'verbose': 0\n",
    "}\n",
    "\n",
    "train_data = lgb.Dataset(X_train,\n",
    "                          label=y_train_binaria2,\n",
    "                          weight=w_train)\n",
    "\n",
    "model = lgb.train(params,\n",
    "                  train_data,\n",
    "                  num_boost_round=best_iter)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iOyqa5mbFySM"
   },
   "source": [
    "Observamos la variables más importantes para el modelo:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "xUejb7eutd0i",
    "outputId": "bdcd9641-679e-4de5-d0f8-cffdfc3bf241"
   },
   "outputs": [],
   "source": [
    "lgb.plot_importance(model, figsize=(10, 20))\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LkTH9daXF5tp"
   },
   "source": [
    "Y si queremos tener las variables más importantes en forma de *Dataframe*:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 424
    },
    "id": "l7ZObpkHtnUl",
    "outputId": "eb0fe3a2-9e41-4886-ebd7-257dd1cf9829"
   },
   "outputs": [],
   "source": [
    "importances = model.feature_importance()\n",
    "feature_names = X_train.columns.tolist()\n",
    "importance_df = pd.DataFrame({'feature': feature_names, 'importance': importances})\n",
    "importance_df = importance_df.sort_values('importance', ascending=False)\n",
    "importance_df[importance_df['importance'] > 0]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pwvqxqc_GB-C"
   },
   "source": [
    "Para guardar el modelo para poder utilizarlo más adelante, no es necesario guardarlo como *pickle*, la librería nos permite guardarlo en formato texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0lWWxwHhs2gp"
   },
   "outputs": [],
   "source": [
    "model.save_model(modelos_path + 'lgb_second.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iHIUeAdsGOWv"
   },
   "source": [
    "Y recuperar el mismo desde ese formato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H12h5wP2s645"
   },
   "outputs": [],
   "source": [
    "model = lgb.Booster(model_file=modelos_path + 'lgb_second.txt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "y_-xspdWGSwT"
   },
   "source": [
    "Para realizar nuestra habitual comparación de modelos, partiremos desde el mejor que obtuvimos hasta ahora, el **rf**. Para este fin cargaremos el *binario* que ajustamos un par de clases atrás:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "J31X1oeRwxbt"
   },
   "outputs": [],
   "source": [
    "#filename_rf_1000 = modelos_path + 'exp_206_random_forest_model_1000.sav'\n",
    "#model_rf_1000 = pickle.load(open(filename_rf_1000, 'rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "28TQpPlIGi6a"
   },
   "source": [
    "Y sobre ambos modelos obtenemos la predicción de **Abril**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kL9dBAv4xWz2"
   },
   "outputs": [],
   "source": [
    "#y_pred_rf = model_rf_1000.predict_proba(Xif)\n",
    "#y_pred_rf = y_pred_rf[:,1] # adaptamos la salida para que sea homogénea con el LGBM\n",
    "\n",
    "y_pred_lgm = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_pred_prob = model.predict(X_test)\n",
    "\n",
    "# Convertir a predicciones binarias usando un umbral de 0.030\n",
    "threshold = 0.030\n",
    "y_pred_binary = (y_pred_prob >= threshold).astype(int)\n",
    "\n",
    "# Agregar las columnas de probabilidades y predicciones al DataFrame original\n",
    "X_test['probabilidad'] = y_pred_prob\n",
    "X_test['prediccion'] = y_pred_binary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#valores_unicos_prediccion = predictions_df['prediccion'].unique()\n",
    "#print(\"Valores únicos de prediccion:\", valores_unicos_prediccion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test.prediccion.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filtrar el DataFrame para quedarte solo con 'numero_de_cliente' y 'prediccion'\n",
    "result_df = X_test[['numero_de_cliente', 'prediccion']]\n",
    "\n",
    "# Renombrar la columna 'prediccion' a 'Predicted' si es necesario\n",
    "result_df.rename(columns={'prediccion': 'Predicted'}, inplace=True)\n",
    "\n",
    "# Ver las primeras filas del DataFrame resultante\n",
    "print(result_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Especificar la ruta completa del archivo donde deseas guardar el DataFrame\n",
    "output_file = dataset_path + \"predicciones_v2.csv\"\n",
    "\n",
    "# Guardar el DataFrame como un archivo CSV en la ruta \n",
    "result_df.to_csv(output_file, index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rf = model_rf_1000.predict_proba(Xif)\n",
    "y_pred_rf = y_pred_rf[:,1] # adaptamos la salida para que sea homogénea con el LGBM\n",
    "\n",
    "y_pred_lgm = model.predict(X_test)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
